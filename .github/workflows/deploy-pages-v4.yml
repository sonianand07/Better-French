name: Auto Update French News â€“ v3 & v4

on:
  schedule:
    - cron:  '0 * * * *'  # RE-ENABLED - Back to stable V4 system
  workflow_dispatch:

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: better-french-v4-pipeline
  cancel-in-progress: false  # Don't cancel running workflows

jobs:
  publish:
    runs-on: ubuntu-latest
    env:
      BF_PER_RUN_CAP: 10
      BF_DAILY_CAP: 9999
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      AI_ENGINE_HIGH_MODEL: openai/gpt-4o-mini

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -e ./ai_engine_v3
          pip install -e ./ai_engine_v4

      - name: Run fast v3 pipeline
        run: |
          python -m ai_engine_v3.scripts.fetch_news
          python -m ai_engine_v3.scripts.qualify_news

      - name: Reset v4 pending store
        run: rm -f ai_engine_v4/data/live/pending_articles.json

      # CRITICAL: Load V4's existing enhanced articles BEFORE copy overwrites them
      - name: Preserve V4 enhanced articles
        run: |
          python - <<'PY'
          import json, pathlib
          
          # Preserve V4's existing enhanced articles before copy step overwrites them
          v4_rolling_path = pathlib.Path('ai_engine_v4/website/rolling_articles.json')
          existing_enhanced = []
          enhanced_links = set()
          
          if v4_rolling_path.exists():
              try:
                  v4_data = json.loads(v4_rolling_path.read_text('utf-8'))
                  existing_enhanced = v4_data.get('articles', v4_data) if isinstance(v4_data, dict) else v4_data
                  enhanced_links = {a.get('original_article_link') for a in existing_enhanced if a.get('quality_checked')}
                  print(f"ðŸ“š Preserving {len(existing_enhanced)} existing V4-enhanced articles")
                  
                  # Save preserved articles to temporary file
                  preserved_path = pathlib.Path('v4_preserved_articles.json')
                  preserved_path.write_text(json.dumps(existing_enhanced, ensure_ascii=False, indent=2))
              except Exception as e:
                  print(f"âš ï¸ Could not load existing enhanced articles: {e}")
                  # Create empty file if no existing articles
                  pathlib.Path('v4_preserved_articles.json').write_text('[]')
          else:
              print("ðŸ“­ No existing V4 articles to preserve")
              pathlib.Path('v4_preserved_articles.json').write_text('[]')
          PY

      - name: Copy data into v4 store (exclude rolling_articles.json)
        run: |
          rsync -a ai_engine_v3/data/ ai_engine_v4/data/
          rsync -a --exclude='rolling_articles.json' ai_engine_v3/website/ ai_engine_v4/website/

      # V4 only processes articles V3 JUST published (cap at 15 max)
      - name: Prepare V4 enhancement queue  
        run: |
          python - <<'PY'
          import json, pathlib
          
          # Load V3's new rolling articles
          v3_rolling_path = pathlib.Path('ai_engine_v3/website/rolling_articles.json')
          v3_articles = []
          if v3_rolling_path.exists():
              v3_data = json.loads(v3_rolling_path.read_text('utf-8'))
              v3_articles = v3_data.get('articles', v3_data) if isinstance(v3_data, dict) else v3_data
              print(f"ðŸ“Š V3 has {len(v3_articles)} total articles")
          
          # Load preserved V4 articles
          preserved_path = pathlib.Path('v4_preserved_articles.json')
          existing_enhanced = []
          enhanced_links = set()
          
          if preserved_path.exists():
              existing_enhanced = json.loads(preserved_path.read_text('utf-8'))
              enhanced_links = {a.get('original_article_link') for a in existing_enhanced if a.get('quality_checked')}
              print(f"ðŸ“š Found {len(existing_enhanced)} preserved V4-enhanced articles")
          
          # Find NEW articles from V3 that need V4 enhancement (last 15 only for safety)
          articles_needing_enhancement = [
              a for a in v3_articles[-15:]  # Safety: only last 15 from V3
              if a.get('ai_enhanced', False) and 
                 a.get('original_article_link') not in enhanced_links
          ]
          
          print(f"ðŸ” V4 will enhance {len(articles_needing_enhancement)} NEW articles from V3")
          print(f"ðŸ“Š Total V4 collection will be: {len(existing_enhanced) + len(articles_needing_enhancement)} articles")
          
          # Safety check: Abort if too many articles to process
          if len(articles_needing_enhancement) > 15:
              print(f"ðŸš¨ ERROR: {len(articles_needing_enhancement)} articles to process - this seems wrong!")
              exit(1)
          
          # Save only the articles that need enhancement
          pending_path = pathlib.Path('ai_engine_v4/data/live/pending_articles.json')
          pending_path.write_text(json.dumps({'articles': articles_needing_enhancement}, ensure_ascii=False, indent=2))
          PY

      - name: Test API key before processing
        run: |
          echo "ðŸ”‘ Testing API key..."
          curl -X POST "https://openrouter.ai/api/v1/chat/completions" \
            -H "Authorization: Bearer $OPENROUTER_API_KEY" \
            -H "Content-Type: application/json" \
            -d '{"model":"openai/gpt-4o-mini","messages":[{"role":"user","content":"test"}],"max_tokens":5}' \
            || (echo "âŒ API key failed - aborting" && exit 1)
          echo "âœ… API key works!"

      - name: Run high-tier verifier (v4)
        run: |
          set -e  # Exit on any error
          PYTHONPATH="$GITHUB_WORKSPACE" python scripts/verify_news.py
          # Verify that at least some articles were processed
          if ! grep -q '"quality_checked": true' ai_engine_v4/data/live/pending_articles.json 2>/dev/null; then
            echo "ERROR: No articles were verified! Verifier may have failed."
            exit 1
          fi
          echo "âœ… Verification completed successfully"

      # Merge preserved V4 articles with newly enhanced ones
      - name: Merge V4 enhanced articles
        run: |
          python - <<'PY'
          import json, pathlib
          
          # Load preserved V4 articles
          preserved_path = pathlib.Path('v4_preserved_articles.json')
          preserved_articles = []
          if preserved_path.exists():
              preserved_articles = json.loads(preserved_path.read_text('utf-8'))
              print(f"ðŸ“š Loading {len(preserved_articles)} preserved V4 articles")
          
          # Load newly enhanced articles from pending store
          pending_path = pathlib.Path('ai_engine_v4/data/live/pending_articles.json')
          newly_enhanced = []
          if pending_path.exists():
              pending_data = json.loads(pending_path.read_text('utf-8'))
              newly_enhanced = pending_data.get('articles', pending_data) if isinstance(pending_data, dict) else pending_data
              # Only include successfully enhanced articles
              newly_enhanced = [a for a in newly_enhanced if a.get('quality_checked', False)]
              print(f"ðŸ” Loading {len(newly_enhanced)} newly enhanced articles")
          
          # Merge: preserved + newly enhanced (avoid duplicates by link)
          all_articles = {}
          
          # Add preserved articles first
          for article in preserved_articles:
              link = article.get('original_article_link')
              if link:
                  all_articles[link] = article
          
          # Add newly enhanced articles (will overwrite if same link)
          for article in newly_enhanced:
              link = article.get('original_article_link')
              if link:
                  all_articles[link] = article
          
          # Convert back to list and sort by date (newest first)
          merged_articles = list(all_articles.values())
          merged_articles.sort(key=lambda x: x.get('original_article_published_date', ''), reverse=True)
          
          # Save merged articles as V4's rolling feed
          v4_rolling_path = pathlib.Path('ai_engine_v4/website/rolling_articles.json')
          final_data = {
              "metadata": {
                  "total_articles": len(merged_articles),
                  "last_updated": "just_now",
                  "v4_enhanced": True
              },
              "articles": merged_articles
          }
          v4_rolling_path.write_text(json.dumps(final_data, ensure_ascii=False, indent=2))
          
          print(f"âœ… Created V4 rolling feed with {len(merged_articles)} total articles")
          print(f"ðŸ“Š {len(preserved_articles)} preserved + {len(newly_enhanced)} new = {len(merged_articles)} total")
          
          # Cleanup temporary file
          if preserved_path.exists():
              preserved_path.unlink()
          PY

      - name: Force cache-busting update
        run: |
          echo "ðŸ”„ Updating cache-busting timestamps..."
          python scripts/auto_cache_bust.py
          echo "âœ… Cache-busting updated - website will force fresh content!"

      - name: Commit & push data changes
        run: |
          git config --global user.email "bot@betterfrench.io"
          git config --global user.name "BetterFrenchBot"
          git config --global core.hooksPath /dev/null
          git add ai_engine_v4/data ai_engine_v4/website || true
          if ! git diff --cached --quiet; then
            git commit -m "chore(v4): auto-update with verification" --no-verify
            git push
          fi

      # Deploy V3 to root site first
      - name: Deploy V3 to Root Site ðŸš€
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_branch: gh-pages
          publish_dir: ./ai_engine_v3/website
          force_orphan: false
          keep_files: true  # Keep v4-site subdirectory
          commit_message: 'chore(deploy): update v3 root site with new articles' 

      # Deploy V4 to /v4-site/
      - name: Deploy V4 Enhanced Site ðŸš€
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_branch: gh-pages
          publish_dir: ./ai_engine_v4/website
          destination_dir: v4-site
          force_orphan: false
          commit_message: 'chore(deploy): update v4 enhanced site with verified articles' 
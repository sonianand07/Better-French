name: V5 - Website Processor

on:
  # schedule:
  #   - cron: '*/30 * * * *'  # Every 30 minutes (DISABLED for development)
  workflow_dispatch:       # Manual trigger only

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: v5-website-processor
  cancel-in-progress: false

jobs:
  process_website:
    runs-on: ubuntu-latest
    
    env:
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      OPENROUTER_SCRAPER_API_KEY: ${{ secrets.OPENROUTER_SCRAPER_API_KEY }}
      PYTHONPATH: ${{ github.workspace }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install aiohttp feedparser requests

      - name: üîç Detect new articles from Rony
        run: |
          python3 - <<'EOF'
          import json
          import os
          from pathlib import Path
          from datetime import datetime
          
          print("üîç V5 WEBSITE PROCESSOR - Detecting New Articles")
          print("=" * 50)
          
          # Check if Rony has collected articles
          data_file = Path('ai_engine_v5/data/scraper_data.json')
          if not data_file.exists():
              print("üì≠ No data from Rony yet - scraper hasn't run")
              with open('process_needed.txt', 'w') as f:
                  f.write('false')
              exit(0)
          
          scraper_data = json.loads(data_file.read_text())
          
          if not scraper_data.get('scraper_runs'):
              print("üì≠ No scraper runs found")
              with open('process_needed.txt', 'w') as f:
                  f.write('false')
              exit(0)
          
          # Find unprocessed runs (new articles)
          unprocessed_runs = []
          for run in scraper_data['scraper_runs']:
              if not run.get('processed_by_website', False):
                  unprocessed_runs.append(run)
          
          total_articles = sum(run['articles_selected'] for run in unprocessed_runs)
          
          print(f"üìä Found {len(unprocessed_runs)} unprocessed runs")
          print(f"üìÑ Total new articles: {total_articles}")
          
          # Show human-readable summary of what's available
          if len(unprocessed_runs) > 0:
              print("\nüóûÔ∏è  ARTICLE OVERVIEW FROM RONY:")
              print("=" * 50)
              
              for i, run in enumerate(unprocessed_runs[-3:], 1):  # Show latest 3 runs
                  timestamp = run.get('timestamp', 'Unknown time')
                  articles_count = run.get('articles_selected', 0)
                  avg_score = run.get('quality_metrics', {}).get('avg_v3_score', 0)
                  
                  print(f"\nüìÖ Run #{i}: {timestamp}")
                  print(f"   üìä Articles: {articles_count} selected (avg quality: {avg_score:.1f})")
                  
                  # Show sample article titles from this run
                  if 'selected_articles' in run and run['selected_articles']:
                      print("   üèÜ Sample Articles:")
                      for j, article in enumerate(run['selected_articles'][:3], 1):
                          title = article.get('title', 'No title')[:60]
                          source = article.get('source', 'Unknown')
                          score = article.get('total_score', 0)
                          print(f"      {j}. \"{title}...\" ({source}, Score: {score:.1f})")
                      
                      if len(run['selected_articles']) > 3:
                          remaining = len(run['selected_articles']) - 3
                          print(f"      ... and {remaining} more articles")
          
          if len(unprocessed_runs) == 0:
              print("‚úÖ All articles are up to date")
              with open('process_needed.txt', 'w') as f:
                  f.write('false')
              exit(0)
          
          # Safety cap - don't process too many at once
          if total_articles > 30:
              print(f"\n‚ö†Ô∏è SAFETY CAP: {total_articles} articles found")
              print("   üõ°Ô∏è Processing latest 3 runs only (max 30 articles)")
              print("   üí° This prevents overwhelming the enhancement pipeline")
              unprocessed_runs = unprocessed_runs[-3:]
              total_articles = sum(run['articles_selected'] for run in unprocessed_runs)
          
          print(f"\nüöÄ PROCESSING QUEUE:")
          print(f"   üìä Runs to process: {len(unprocessed_runs)}")
          print(f"   üìÑ Total articles: {total_articles}")
          print("   üéØ Each article will get V3+V4 enhancement")
          
          # Save processing queue with human-readable article info
          articles_for_processing = []
          for run in unprocessed_runs:
              if 'selected_articles' in run:
                  articles_for_processing.extend(run['selected_articles'])
          
          # Show the actual articles that will be processed
          print(f"\nüìã ARTICLES ENTERING V3+V4 PIPELINE:")
          print("=" * 50)
          for i, article in enumerate(articles_for_processing[:10], 1):
              title = article.get('title', 'No title')[:50]
              source = article.get('source', 'Unknown')[:15]
              score = article.get('total_score', 0)
              print(f"   {i:2}. \"{title}...\" ({source}, Score: {score:.1f})")
          
          if len(articles_for_processing) > 10:
              remaining = len(articles_for_processing) - 10
              print(f"   ... and {remaining} more articles")
          
          with open('processing_queue.json', 'w') as f:
              json.dump({
                  'runs_to_process': unprocessed_runs,
                  'total_articles': total_articles,
                  'articles_for_processing': articles_for_processing
              }, f, indent=2)
          
          with open('process_needed.txt', 'w') as f:
              f.write('true')
          
          print(f"\n‚úÖ Ready to enhance {total_articles} articles with V3+V4 pipeline!")
          EOF

      - name: ü§ñ Apply V3+V4 Enhancement Pipeline
        run: |
          if [ "$(cat process_needed.txt)" = "true" ]; then
            echo "ü§ñ APPLYING V3+V4 ENHANCEMENT TO RONY'S ARTICLES"
            echo "=" * 50
            
            python3 - <<'EOF'
          import json
          import sys
          import os
          from pathlib import Path
          from datetime import datetime
          
          # Load processing queue
          with open('processing_queue.json', 'r') as f:
              queue = json.load(f)
          
          runs_to_process = queue['runs_to_process']
          articles_for_processing = queue['articles_for_processing']
          
          print(f"üéØ ENHANCEMENT PIPELINE STARTING")
          print(f"   üìä Runs: {len(runs_to_process)}")
          print(f"   üìÑ Articles: {len(articles_for_processing)}")
          print(f"   üîß Method: PROVEN V3+V4 prompts (no quality reduction)")
          print("")
          
          # Show what will be enhanced
          print("üîß ARTICLES ENTERING ENHANCEMENT:")
          for i, article in enumerate(articles_for_processing[:5], 1):
              title = article.get('title', 'No title')[:55]
              source = article.get('source', 'Unknown')
              score = article.get('total_score', 0)
              print(f"   {i}. \"{title}...\"")
              print(f"      üìä Source: {source} | Quality: {score:.1f}/25")
          
          if len(articles_for_processing) > 5:
              remaining = len(articles_for_processing) - 5
              print(f"   ... and {remaining} more articles")
          
          print("")
          
          # Apply REAL V3+V4 enhancement pipeline
          try:
              print("üîç Loading V5 WebsiteProcessor...")
              sys.path.append('ai_engine_v5')
              from core.processor.website_processor import WebsiteProcessor
              
              print("‚úÖ V5 WebsiteProcessor loaded successfully")
              print("üéØ Initializing with PROVEN V3+V4 components...")
              
              # Use V5's REAL processor (preserves V3+V4 quality)
              processor = WebsiteProcessor()
              
              print(f"\nüöÄ STARTING ENHANCEMENT OF {len(articles_for_processing)} ARTICLES")
              print("=" * 60)
              print("üìã Pipeline Steps:")
              print("   1Ô∏è‚É£ Convert to V3 format")
              print("   2Ô∏è‚É£ Apply V3 contextual analysis (contextual_words_v3.jinja)")  
              print("   3Ô∏è‚É£ Apply V3 simplification (simplify_titles_summaries_v3.jinja)")
              print("   4Ô∏è‚É£ Apply V4 GPT-4o verification (review_tooltips.jinja)")
              print("   5Ô∏è‚É£ Generate beautiful website")
              print("")
              
              enhanced_articles, enhancement_cost = processor.enhance_articles(articles_for_processing)
              
              print(f"\n‚úÖ ENHANCEMENT COMPLETE!")
              print(f"   üìä Articles enhanced: {len(enhanced_articles)}")
              print(f"   üí∞ Total cost: ${enhancement_cost:.4f}")
              
              # Generate website with V3+V4 enhanced articles
              print(f"\nüåê GENERATING WEBSITE...")
              website_result = processor.generate_website(enhanced_articles)
              
              print(f"‚úÖ WEBSITE GENERATED!")
              print(f"   üìÅ Files created: {website_result.get('files_created', 'Unknown')}")
              print(f"   üé® Theme: Modern French learning interface")
              print(f"   üì± Features: Tooltips, responsive design, quality indicators")
              
          except Exception as e:
              print(f"‚ùå ENHANCEMENT FAILED: {e}")
              print("üîß Creating fallback website...")
              
              # Fallback: create simple website with basic info
              website_dir = Path('ai_engine_v5/website')
              website_dir.mkdir(parents=True, exist_ok=True)
              
              # Create a more informative fallback page
              article_list = ""
              for i, article in enumerate(articles_for_processing[:10], 1):
                  title = article.get('title', 'No title')
                  source = article.get('source', 'Unknown')
                  link = article.get('link', '#')
                  score = article.get('total_score', 0)
                  article_list += f"""
                  <div class="article">
                      <h3><a href="{link}" target="_blank">{title}</a></h3>
                      <p><strong>Source:</strong> {source} | <strong>Quality:</strong> {score:.1f}/25</p>
                  </div>
                  """
              
              basic_html = f'''<!DOCTYPE html>
              <html lang="fr">
              <head>
                  <meta charset="UTF-8">
                  <meta name="viewport" content="width=device-width, initial-scale=1.0">
                  <title>Better French V5 - Fallback Mode</title>
                  <style>
                      body {{ font-family: -apple-system, BlinkMacSystemFont, sans-serif; margin: 40px; }}
                      .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 8px; }}
                      .article {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 5px; }}
                      .article h3 {{ margin: 0 0 10px 0; }}
                      .article a {{ color: #2563eb; text-decoration: none; }}
                      .stats {{ background: #f8fafc; padding: 15px; border-radius: 8px; margin: 20px 0; }}
                  </style>
              </head>
              <body>
                  <div class="header">
                      <h1>ü§ñ Better French V5 - Fallback Mode</h1>
                      <p>Articles collected by Rony autonomous scraper</p>
                  </div>
                  
                  <div class="stats">
                      <h2>üìä Collection Stats</h2>
                      <p><strong>Articles collected:</strong> {len(articles_for_processing)}</p>
                      <p><strong>Status:</strong> V3+V4 enhancement temporarily unavailable</p>
                      <p><strong>Next step:</strong> Check back soon for full enhancement!</p>
                  </div>
                  
                  <h2>üì∞ Latest Articles from Rony</h2>
                  {article_list}
                  
                  <p><small>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}</small></p>
              </body></html>'''
              
              with open('ai_engine_v5/website/index.html', 'w', encoding='utf-8') as f:
                  f.write(basic_html)
              
              print(f"‚úÖ Fallback website created with {len(articles_for_processing)} articles")
          
          # Mark runs as processed
          data_file = Path('ai_engine_v5/data/scraper_data.json')
          scraper_data = json.loads(data_file.read_text())
          
          processed_timestamps = {run['timestamp'] for run in runs_to_process}
          processed_count = 0
          for run in scraper_data['scraper_runs']:
              if run['timestamp'] in processed_timestamps:
                  run['processed_by_website'] = True
                  processed_count += 1
          
          data_file.write_text(json.dumps(scraper_data, indent=2, ensure_ascii=False))
          
          print(f"\n‚úÖ PROCESSING COMPLETE!")
          print(f"   ‚úÖ {processed_count} runs marked as processed")
          print(f"   üåê Website ready for deployment")
          print(f"   üìä Articles enhanced and ready to view")
          EOF
          else
            echo "‚ÑπÔ∏è No new articles to process"
          fi

      - name: üöÄ Deploy V5 Website
        if: success()
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_branch: gh-pages
          publish_dir: ./ai_engine_v5/website
          destination_dir: v5-site
          commit_message: |
            üåê V5 website update - Rony's enhanced articles
            
            ‚úÖ Profile-aware article selection by Rony
            ü§ñ V3+V4 enhancement pipeline applied
            üìä Latest high-quality French learning content

      - name: üìù Commit processing results
        run: |
          git config --global user.email "v5-processor@betterfrench.io"
          git config --global user.name "V5-WebsiteProcessor"
          git config --global core.hooksPath /dev/null
          
          git add ai_engine_v5/data/ || true
          
          if ! git diff --cached --quiet; then
            TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M UTC")
            git commit -m "üåê V5 website processing: ${TIMESTAMP}
            
            ‚úÖ Articles enhanced with V3+V4 pipeline
            üéØ Profile-aware content selection by Rony  
            üìä High-quality French learning articles processed
            üöÄ Website deployed to /v5-site" --no-verify
            
            git push
            echo "‚úÖ Processing results committed successfully"
          else
            echo "‚ÑπÔ∏è No processing data changes to commit"
          fi

      - name: üéâ Summary
        run: |
          echo "üéâ V5 WEBSITE PROCESSOR COMPLETE!"
          echo "=================================="
          echo ""
          echo "üåê Your V5 website is now live at:"
          echo "   https://sonianand07.github.io/Better-French/v5-site/"
          echo ""
          echo "ü§ñ Pipeline applied:"
          echo "   1Ô∏è‚É£ Rony intelligent article selection"
          echo "   2Ô∏è‚É£ V3 contextual analysis & simplification"
          echo "   3Ô∏è‚É£ V4 GPT-4o verification"
          echo "   4Ô∏è‚É£ V5 modern website generation"
          echo ""
          echo "‚ú® Features:"
          echo "   üì± Responsive design"
          echo "   üéØ French learning tooltips"
          echo "   üìä Quality indicators"
          echo "   üîÑ Profile-aware content"
          echo ""
          echo "üîÑ Next run: Manual trigger or scheduled"

      - name: Clean up
        run: |
          rm -f process_needed.txt processing_queue.json || true 
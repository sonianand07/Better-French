name: V5 - Website Processor

on:
  schedule:
    - cron: '*/30 * * * *'  # Every 30 minutes - process new articles from Rony
  workflow_dispatch:       # Manual trigger for testing

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: v5-website-processor
  cancel-in-progress: false

jobs:
  process_website:
    runs-on: ubuntu-latest
    
    env:
      BF_PER_RUN_CAP: 15  # Increased from 10 to 15 as requested
      BF_DAILY_CAP: 9999
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      OPENROUTER_SCRAPER_API_KEY: ${{ secrets.OPENROUTER_SCRAPER_API_KEY }}
      AI_ENGINE_HIGH_MODEL: openai/gpt-4o-mini
      PYTHONPATH: ${{ github.workspace }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install aiohttp feedparser requests
          pip install -e ./ai_engine_v3
          pip install -e ./ai_engine_v4

      - name: üîç Detect new articles from Rony
        run: |
          python3 - <<'EOF'
          import json
          import os
          from pathlib import Path
          from datetime import datetime
          
          print("üîç V5 WEBSITE PROCESSOR - Detecting New Articles")
          print("=" * 50)
          
          # Check if Rony has collected articles
          data_file = Path('ai_engine_v5/data/scraper_data.json')
          if not data_file.exists():
              print("üì≠ No data from Rony yet - scraper hasn't run")
              with open('process_needed.txt', 'w') as f:
                  f.write('false')
              exit(0)
          
          scraper_data = json.loads(data_file.read_text())
          
          if not scraper_data.get('scraper_runs'):
              print("üì≠ No scraper runs found")
              with open('process_needed.txt', 'w') as f:
                  f.write('false')
              exit(0)
          
          # Find unprocessed runs (new articles)
          unprocessed_runs = []
          for run in scraper_data['scraper_runs']:
              if not run.get('processed_by_website', False):
                  unprocessed_runs.append(run)
          
          total_articles = sum(run['articles_selected'] for run in unprocessed_runs)
          
          print(f"üìä Found {len(unprocessed_runs)} unprocessed runs")
          print(f"üìÑ Total new articles: {total_articles}")
          
          # Show human-readable summary of what's available
          if len(unprocessed_runs) > 0:
              print("\nüóûÔ∏è  ARTICLE OVERVIEW FROM RONY:")
              print("=" * 50)
              
              for i, run in enumerate(unprocessed_runs[-3:], 1):  # Show latest 3 runs
                  timestamp = run.get('timestamp', 'Unknown time')
                  articles_count = run.get('articles_selected', 0)
                  avg_score = run.get('quality_metrics', {}).get('avg_v3_score', 0)
                  
                  print(f"\nüìÖ Run #{i}: {timestamp}")
                  print(f"   üìä Articles: {articles_count} selected (avg quality: {avg_score:.1f})")
                  
                  # Show sample article titles from this run
                  if 'selected_articles' in run and run['selected_articles']:
                      print("   üèÜ Sample Articles:")
                      for j, article in enumerate(run['selected_articles'][:3], 1):
                          title = article.get('title', 'No title')[:60]
                          source = article.get('source', 'Unknown')
                          score = article.get('total_score', 0)
                          print(f"      {j}. \"{title}...\" ({source}, Score: {score:.1f})")
                      
                      if len(run['selected_articles']) > 3:
                          remaining = len(run['selected_articles']) - 3
                          print(f"      ... and {remaining} more articles")
          
          if len(unprocessed_runs) == 0:
              print("‚úÖ All articles are up to date")
              with open('process_needed.txt', 'w') as f:
                  f.write('false')
              exit(0)
          
          # Safety cap - process manageable batches for immediate feedback
          # Increase cap from 5 ‚ûú 15 so a single run can publish a fuller page while
          # still preventing huge backlogs from slowing down the workflow.
          if total_articles > 15:
              print(f"\n‚ö†Ô∏è SAFETY CAP: {total_articles} articles found")
              print("   üõ°Ô∏è Processing latest batch only (max 15 articles)")
              print("   üí° Small batches for immediate feedback and faster processing")
              # Take only enough runs to get ~5 articles
              selected_runs = []
              article_count = 0
              for run in reversed(unprocessed_runs):
                  selected_runs.insert(0, run)
                  article_count += run.get('articles_selected', 0)
                  if article_count >= 15:
                      break
              unprocessed_runs = selected_runs
              total_articles = sum(run['articles_selected'] for run in unprocessed_runs)
          
          print(f"\nüöÄ PROCESSING QUEUE:")
          print(f"   üìä Runs to process: {len(unprocessed_runs)}")
          print(f"   üìÑ Total articles: {total_articles}")
          print("   üéØ Each article will get V3+V4 enhancement")
          
          # Save processing queue with human-readable article info
          articles_for_processing = []
          for run in unprocessed_runs:
              if 'selected_articles' in run:
                  articles_for_processing.extend(run['selected_articles'])
          
          # Show the actual articles that will be processed
          print(f"\nüìã ARTICLES ENTERING V3+V4 PIPELINE:")
          print("=" * 50)
          for i, article in enumerate(articles_for_processing[:10], 1):
              title = article.get('title', 'No title')[:50]
              source = article.get('source', 'Unknown')[:15]
              score = article.get('total_score', 0)
              print(f"   {i:2}. \"{title}...\" ({source}, Score: {score:.1f})")
          
          if len(articles_for_processing) > 10:
              remaining = len(articles_for_processing) - 10
              print(f"   ... and {remaining} more articles")
          
          # Show IMMEDIATE article processing preview
          print(f"\nüöÄ IMMEDIATE PROCESSING PREVIEW:")
          print("=" * 50)
          print(f"üìä These {min(len(articles_for_processing), 5)} articles will be enhanced:")
          for i, article in enumerate(articles_for_processing[:5], 1):
              title = article.get('title', 'No title')[:60]
              source = article.get('source', 'Unknown')
              score = article.get('total_score', 0)
              print(f"   {i}. üîß Processing: \"{title}...\"")
              print(f"      üìä Source: {source} | Quality: {score:.1f}/25")
              print(f"      üéØ Will get: Contextual analysis ‚Üí Simplification ‚Üí GPT-4o verification")
              
          print(f"\n‚ö° Small batch processing: {len(articles_for_processing)} articles")
          print(f"üí∞ Estimated cost: ~${len(articles_for_processing) * 0.012:.3f}")
          print(f"‚è±Ô∏è Estimated time: ~{len(articles_for_processing) * 30} seconds")
          print(f"üéØ Next: Starting AI enhancement pipeline...")
          
          with open('processing_queue.json', 'w') as f:
              json.dump({
                  'runs_to_process': unprocessed_runs,
                  'total_articles': total_articles,
                  'articles_for_processing': articles_for_processing
              }, f, indent=2)
          
          with open('process_needed.txt', 'w') as f:
              f.write('true')
          
          print(f"\n‚úÖ Ready to enhance {total_articles} articles with V3+V4 pipeline!")
          EOF

      - name: ü§ñ Apply V3+V4 Enhancement Pipeline
        run: |
          if [ "$(cat process_needed.txt)" = "true" ]; then
            echo "ü§ñ APPLYING V3+V4 ENHANCEMENT TO RONY'S ARTICLES"
            echo "=" * 50
            
            python3 - <<'EOF'
          import json
          import sys
          import os
          from pathlib import Path
          from datetime import datetime
          
          # Load processing queue
          with open('processing_queue.json', 'r') as f:
              queue = json.load(f)
          
          runs_to_process = queue['runs_to_process']
          articles_for_processing = queue['articles_for_processing']
          
          print(f"üéØ ENHANCEMENT PIPELINE STARTING")
          print(f"   üìä Runs: {len(runs_to_process)}")
          print(f"   üìÑ Articles: {len(articles_for_processing)}")
          print(f"   üîß Method: PROVEN V3+V4 prompts (no quality reduction)")
          print("")
          
          # Show what will be enhanced
          print("üîß ARTICLES ENTERING ENHANCEMENT:")
          for i, article in enumerate(articles_for_processing[:5], 1):
              title = article.get('title', 'No title')[:55]
              source = article.get('source', 'Unknown')
              score = article.get('total_score', 0)
              print(f"   {i}. \"{title}...\"")
              print(f"      üìä Source: {source} | Quality: {score:.1f}/25")
          
          if len(articles_for_processing) > 5:
              remaining = len(articles_for_processing) - 5
              print(f"   ... and {remaining} more articles")
          
          print("")
          
          # Apply REAL V3+V4 enhancement pipeline
          try:
              print("üîç Loading V5 WebsiteProcessor...")
              sys.path.append('ai_engine_v5')
              from core.processor.website_processor import WebsiteProcessor
              
              print("‚úÖ V5 WebsiteProcessor loaded successfully")
              print("üéØ Initializing with PROVEN V3+V4 components...")
              
              # Use V5's REAL processor (preserves V3+V4 quality)
              processor = WebsiteProcessor()
              
              print(f"\nüöÄ STARTING ENHANCEMENT OF {len(articles_for_processing)} ARTICLES")
              print("=" * 60)
              print("üìã Pipeline Steps:")
              print("   1Ô∏è‚É£ Convert to V3 format")
              print("   2Ô∏è‚É£ Apply V3 contextual analysis (contextual_words_v3.jinja)")  
              print("   3Ô∏è‚É£ Apply V3 simplification (simplify_titles_summaries_v3.jinja)")
              print("   4Ô∏è‚É£ Apply V4 GPT-4o verification (review_tooltips.jinja)")
              print("   5Ô∏è‚É£ Generate beautiful website")
              print("")
              
              enhanced_articles, enhancement_cost = processor.enhance_articles(articles_for_processing)
              
              print(f"\n‚úÖ ENHANCEMENT COMPLETE!")
              print(f"   üìä Articles enhanced: {len(enhanced_articles)}")
              print(f"   üí∞ Total cost: ${enhancement_cost:.4f}")
              
              # Generate website with V3+V4 enhanced articles
              print(f"\nüåê GENERATING WEBSITE...")
              website_result = processor.generate_website(enhanced_articles)
              
              print(f"‚úÖ WEBSITE GENERATED!")
              print(f"   üìÅ Files created: {website_result.get('files_created', 'Unknown')}")
              print(f"   üé® Theme: Modern French learning interface")
              print(f"   üì± Features: Tooltips, responsive design, quality indicators")
              
          except Exception as e:
              print(f"‚ùå ENHANCEMENT FAILED: {e}")
              print("üö® NO FALLBACK MODE - FAILING PROPERLY!")
              print("üí° Fix the V5 WebsiteProcessor to resolve this issue")
              print("‚ùå Workflow will fail to prevent fake website generation")
              
              # Don't create fallback - let it fail properly
              raise Exception(f"V5 enhancement failed: {e}")
          
          # Mark runs as processed
          data_file = Path('ai_engine_v5/data/scraper_data.json')
          scraper_data = json.loads(data_file.read_text())
          
          processed_timestamps = {run['timestamp'] for run in runs_to_process}
          processed_count = 0
          for run in scraper_data['scraper_runs']:
              if run['timestamp'] in processed_timestamps:
                  run['processed_by_website'] = True
                  processed_count += 1
          
          data_file.write_text(json.dumps(scraper_data, indent=2, ensure_ascii=False))
          
          print(f"\n‚úÖ PROCESSING COMPLETE!")
          print(f"   ‚úÖ {processed_count} runs marked as processed")
          print(f"   üåê Website ready for deployment")
          print(f"   üìä Articles enhanced and ready to view")
          EOF
          else
            echo "‚ÑπÔ∏è No new articles to process"
          fi

      - name: üöÄ Deploy V5 Website
        if: success()
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_branch: gh-pages
          publish_dir: ./ai_engine_v5/website
          destination_dir: v5-site
          force_orphan: false
          keep_files: true
          commit_message: |
            üåê V5 website update - Rony's enhanced articles
            
            ‚úÖ Profile-aware article selection by Rony
            ü§ñ V3+V4 enhancement pipeline applied
            üìä Latest high-quality French learning content
            
      - name: ‚úÖ Verify V5 Deployment
        run: |
          echo "üîç Verifying V5 website deployment..."
          
          # Wait a moment for deployment to propagate
          sleep 10
          
          # Test if the website is accessible
          if curl -f -s "https://sonianand07.github.io/Better-French/v5-site/" > /dev/null; then
            echo "‚úÖ V5 website is live and accessible!"
          else
            echo "‚ö†Ô∏è V5 website might take a few minutes to propagate"
          fi
          
          echo "üåê V5 Website URL: https://sonianand07.github.io/Better-French/v5-site/"
          echo "üìä Expected: Enhanced articles with French learning tooltips"

      - name: üìù Commit processing results
        run: |
          git config --global user.email "v5-processor@betterfrench.io"
          git config --global user.name "V5-WebsiteProcessor"
          git config --global core.hooksPath /dev/null
          
          git add ai_engine_v5/data/ || true
          
          if ! git diff --cached --quiet; then
            TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M UTC")
            git commit -m "üåê V5 website processing: ${TIMESTAMP}
            
            ‚úÖ Articles enhanced with V3+V4 pipeline
            üéØ Profile-aware content selection by Rony  
            üìä High-quality French learning articles processed
            üöÄ Website deployed to /v5-site" --no-verify
            
            git push
            echo "‚úÖ Processing results committed successfully"
          else
            echo "‚ÑπÔ∏è No processing data changes to commit"
          fi

      - name: üéâ Summary
        run: |
          echo "üéâ V5 WEBSITE PROCESSOR COMPLETE!"
          echo "=================================="
          echo ""
          echo "üåê Your V5 website is now live at:"
          echo "   https://sonianand07.github.io/Better-French/v5-site/"
          echo ""
          echo "ü§ñ Pipeline applied:"
          echo "   1Ô∏è‚É£ Rony intelligent article selection"
          echo "   2Ô∏è‚É£ V3 contextual analysis & simplification"
          echo "   3Ô∏è‚É£ V4 GPT-4o verification"
          echo "   4Ô∏è‚É£ V5 modern website generation"
          echo ""
          echo "‚ú® Features:"
          echo "   üì± Responsive design"
          echo "   üéØ French learning tooltips"
          echo "   üìä Quality indicators"
          echo "   üîÑ Profile-aware content"
          echo ""
          echo "üîÑ Next run: Manual trigger or scheduled"

      - name: Clean up
        run: |
          rm -f process_needed.txt processing_queue.json || true 
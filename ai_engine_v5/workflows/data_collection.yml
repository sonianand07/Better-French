name: V5 Autonomous Scraper (Hourly)

on:
  schedule:
    - cron: '0 * * * *'  # Every hour (Paris time aware)
  workflow_dispatch:  # Manual trigger for testing

permissions:
  contents: write

concurrency:
  group: ai-engine-v5-autonomous-scraper
  cancel-in-progress: false

jobs:
  autonomous_scrape:
    runs-on: ubuntu-latest
    env:
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      # Minimal LLM usage - Gemini 2.5 Flash for selection only
      AI_ENGINE_SELECTION_MODEL: google/gemini-2.0-flash-exp
      BF_PER_RUN_CAP: 150  # Collect candidates for LLM selection
      
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -e ./ai_engine_v5  # Self-contained v5 only

      # V5 AUTONOMOUS SCRAPER - Minimal & Fast
      - name: ü§ñ Run Autonomous Scraper
        run: |
          python - <<'PY'
          import json
          import sys
          import os
          from datetime import datetime, timezone
          from pathlib import Path
          
          print("ü§ñ V5 AUTONOMOUS SCRAPER - MINIMAL & FAST")
          print("=" * 50)
          
          # Check if current hour already processed
          paris_tz = timezone.utc  # For now, using UTC (can be adjusted)
          current_hour = datetime.now(paris_tz).replace(minute=0, second=0, microsecond=0)
          hour_key = current_hour.isoformat()
          
          # Load scraper data file (single file approach)
          data_file = Path('ai_engine_v5/data/scraper_data.json')
          data_file.parent.mkdir(parents=True, exist_ok=True)
          
          if data_file.exists():
              scraper_data = json.loads(data_file.read_text('utf-8'))
          else:
              scraper_data = {
                  "scraper_state": {
                      "last_processed_hour": "",
                      "total_hours_processed": 0,
                      "last_run_status": "never_run"
                  },
                  "collected_articles": []
              }
          
          # Check if this hour already processed
          if hour_key == scraper_data["scraper_state"]["last_processed_hour"]:
              print(f"‚è≠Ô∏è Hour {hour_key} already processed - skipping")
              exit(0)
          
          print(f"üïê Processing hour: {hour_key}")
          
          # MINIMAL SCRAPING - reuse existing logic but simplified
          sys.path.append('ai_engine_v5')
          
          try:
              # Import v5's self-contained scraper (will create this)
              from ai_engine_v5.core.scraper.autonomous_scraper import AutonomousScraper
              
              scraper = AutonomousScraper()
              
              # Step 1: Get candidate articles for this hour
              print("üì° Scraping articles for current hour...")
              candidates = scraper.scrape_current_hour()
              print(f"üìÑ Found {len(candidates)} candidate articles")
              
              if len(candidates) < 5:
                  print("‚ö†Ô∏è Too few articles found - will try again next hour")
                  scraper_data["scraper_state"]["last_run_status"] = "insufficient_articles"
                  data_file.write_text(json.dumps(scraper_data, indent=2))
                  exit(0)
              
              # Step 2: Use Gemini 2.5 Flash to select top 10
              print("üß† Using LLM to select top 10 articles...")
              selection_result = scraper.llm_select_top_10(candidates)
              
              print(f"‚úÖ Selected {len(selection_result['selected_articles'])} articles")
              print(f"üí∞ LLM cost: ${selection_result['cost']:.3f}")
              
              # Step 3: Update scraper data file
              new_hour_data = {
                  "hour": hour_key,
                  "articles": selection_result['selected_articles'],
                  "selection_reasoning": selection_result['reasoning'],
                  "processed_by_website": False,
                  "cost": selection_result['cost'],
                  "candidate_count": len(candidates)
              }
              
              scraper_data["collected_articles"].append(new_hour_data)
              scraper_data["scraper_state"] = {
                  "last_processed_hour": hour_key,
                  "total_hours_processed": scraper_data["scraper_state"]["total_hours_processed"] + 1,
                  "last_run_status": "success"
              }
              
              # Keep only last 48 hours of data (cleanup)
              if len(scraper_data["collected_articles"]) > 48:
                  scraper_data["collected_articles"] = scraper_data["collected_articles"][-48:]
              
              data_file.write_text(json.dumps(scraper_data, indent=2, ensure_ascii=False))
              
              print("üéâ AUTONOMOUS SCRAPER COMPLETE!")
              print(f"üìä Total hours processed: {scraper_data['scraper_state']['total_hours_processed']}")
              print(f"üì¶ Data stored in: {data_file}")
              
          except ImportError:
              print("‚ö†Ô∏è V5 scraper not yet implemented - creating placeholder")
              # Temporary: create placeholder data for development
              placeholder_articles = [
                  {
                      "title": f"Article {i+1} for {hour_key}",
                      "summary": f"Summary for article {i+1}",
                      "link": f"https://example.com/article{i+1}",
                      "source": "Development Source",
                      "published_date": hour_key
                  }
                  for i in range(10)
              ]
              
              new_hour_data = {
                  "hour": hour_key,
                  "articles": placeholder_articles,
                  "selection_reasoning": "Placeholder data for development",
                  "processed_by_website": False,
                  "cost": 0.0,
                  "candidate_count": 50
              }
              
              scraper_data["collected_articles"].append(new_hour_data)
              scraper_data["scraper_state"] = {
                  "last_processed_hour": hour_key,
                  "total_hours_processed": scraper_data["scraper_state"]["total_hours_processed"] + 1,
                  "last_run_status": "placeholder_mode"
              }
              
              data_file.write_text(json.dumps(scraper_data, indent=2, ensure_ascii=False))
              print("üì¶ Placeholder data created for development")
              
          except Exception as e:
              print(f"‚ùå Scraper failed: {e}")
              scraper_data["scraper_state"]["last_run_status"] = f"error: {str(e)}"
              data_file.write_text(json.dumps(scraper_data, indent=2))
              exit(1)
          PY

      - name: üîÑ Commit scraper data
        run: |
          git config --global user.email "v5-scraper@betterfrench.io"
          git config --global user.name "AI-Engine-v5-AutonomousScraper"
          git config --global core.hooksPath /dev/null
          
          git add ai_engine_v5/data/ || true
          
          if ! git diff --cached --quiet; then
            HOUR=$(date -u +"%Y-%m-%d %H:00 UTC")
            
            git commit -m "ü§ñ v5 autonomous scraper: ${HOUR}
            
            - Processed hourly articles with LLM selection
            - Selected top 10 articles using Gemini 2.5 Flash
            - Stored in autonomous scraper data file
            - Ready for website processor workflow" --no-verify
            
            git push
            
            echo "‚úÖ Scraper data committed and pushed"
          else
            echo "‚ÑπÔ∏è No new scraper data to commit"
          fi 
# 2025-06-23

## Accomplishments (session)

| Task | Note | Status |
|------|------|--------|
| English-heading rule live | Processor switched to `contextual_words_v2.jinja` | ‚úÖ Done |
| Partial acceptance in validator | Bad headings discarded, good ones kept | ‚úÖ Done |
| Retry logic | Explanations prompt now retries 4√ó with explicit example | ‚úÖ Done |
| Coverage pass check | Manual QA shows quality correct but gaps remain | ‚úÖ Done |
| Robustness fix | Validator now ignores bad items where `original_word` is not a string | ‚úÖ Done |

### File-change log
| File | Action |
|------|--------|
| `ai_engine_v3/processor.py` | retry logic + example, max_attempts 4 |
| `ai_engine_v3/validator.py` | partial-accept strategy |
| `ai_engine_v3/validator.py` | guard against non-string `original_word` (prevents "unhashable list" error) |
| `ai_engine_v3/processor.py` | skip any explanation objects with non-string `original_word` |
| `ai_engine_v3/processor.py` | try/except guard when adding to dict to avoid list-key crash |
| `ai_engine_v3/validator.py` | hashability check when building coverage set |
| `ai_engine_v3/processor.py` | log full traceback when an article fails |

## Pain-points
1. Still missing tool-tips when LLM skips tokens.
2. Proper-noun punctuation/hyphen variants can throw off match.

## Next actions
| ID | Action | Owner | Status |
|----|--------|-------|--------|
| N-1 | Create branch `v3-tokenised-context` | AI | üöß In Progress |
| N-2 | Prototype two-stage tokenisation ‚Üí gloss pipeline | AI | ‚ùå Pending |
| N-3 | Measure coverage vs single-prompt baseline | AI | ‚ùå Pending |

## Design sketch ‚Äì token-first strategy

1. **Stage A ‚Äì Tokenise title**
   ‚Ä¢ Prompt: "Here is the article title + (optionally) a one-sentence context block. List **all** distinct tokens worth glossing, preserving original accents/punctuation."  
   ‚Ä¢ Output: JSON list `["Iran-Isra√´l", "Assembl√©e nationale", ‚Ä¶]`.
2. **Stage B ‚Äì Gloss each token** (current prompt with enforced list)
   ‚Ä¢ Provide the returned list as `TOKENS:` and require one object per entry.
3. **Fallback**
   ‚Ä¢ If Stage A fails, fall back to built-in tokenizer for same sentence.

Pros: higher coverage, model thinks about segmentation explicitly.  
Cons: costs ~2√ó tokens + latency.

We will prototype this in the new branch without touching production. 
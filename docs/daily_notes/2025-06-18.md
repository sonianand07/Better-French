# 2025-06-18 â€“ AI-Engine v3 Progress Log

## What we built today

1. **ai_engine_v3 scaffold** â€“ cloned v2 so experimentation won't disturb production.
2. **Two-stage pipeline**  
   â€¢ `scripts/fetch_news.py` â€“ scrapes all sources every 30 min and stores raw JSON.  
   â€¢ `scripts/qualify_news.py` â€“ runs every 1â€“2 h, filters with rules, asks an LLM for relevance, caps to *â‰¤ 50* headlines per day, then calls the full Processor (titles, summaries, glossary).
3. **Relevance LLM** â€“ single-prompt scorer using `anthropic/claude-3.5-sonnet`; cost â‰ˆ $0.04 / 500 headlines (see calculation below).
4. **Bucket balancing** â€“ guarantees ~10 work-domain, ~10 world-affairs, rest France-general headlines in the daily 50.
5. **New French sources** â€“ added Le Figaro, Les Ã‰chos, L'Obs, France-Info, AFP.
6. **Improved ordering** â€“ site now sorts by article *published* date.
7. **Orchestrator** â€“ `scripts/run_v3_pipeline.py` to fetch â†’ qualify and (optionally) serve the v3 site locally.
8. **Smoke test** â€“ `qa/local/check_contextual_words_v3.py` checks glossaries, coverage and ordering for the v3 feed.

## LLM cost estimate for qualification

| Metric | Value |
|--------|-------|
| Headlines scored per run (worst-case) | 500 |
| Prompt tokens / headline | 15 |
| Completion tokens / headline | 2 |
| Claude-Sonnet price (prompt) | $0.003 / 1 k tokens |
| Claude-Sonnet price (completion) | $0.015 / 1 k tokens |
| **Daily cost** | **â‰ˆ $0.038** (â‰ˆ $1.15 / month) |

This is negligible compared with the main processing cost.

## Next LLM opportunities (future work)

* **Headline dedup / clustering** â€“ use embeddings to cluster near-duplicates from different outlets.
* **Automatic keyword tagging** â€“ ask the LLM for 3â€“5 tags per article to power search and category filters.
* **Personal feedback loop** â€“ log clicks/favourites and feed them back to the LLM prompt ("user clicked on X in the past weekâ€¦").

### Profile-summary strategy agreed today

* **Now**: build the sentence deterministically from the JSON (age, work_domains, interests, pain_points). Zero cost, always consistent.
* **Later**: add an optional command that passes the full JSON to an LLM once per profile and caches the nicer wording. Estimated one-off cost <$0.005 per user.

All of these are optional and can be added incrementally once v3 proves stable.

---
*End of day summary prepared automatically by the assistant.*

---
## Implementation checklist added (QA request)

The following safety & UX tasks are queued for the **next commit** and will be marked done once pushed:

1. **API-key fast-fail** â€“ 1-token ping to OpenRouter at startup; abort on failure.
2. **Atomic file writes** â€“ save to `.tmp` then `rename` for both `pending_articles.json` and `rolling_articles.json`.
3. **Smarter local server** â€“ try 8010, else first open port 8011-8020; skip with warning if all busy.
4. **Progress heartbeat** â€“ spinner / phase indicators printed every few seconds during scrape & processing.
5. **Cost line** â€“ always print `ðŸ’° Relevance LLM cost | Processor cost` at end of qualifier.
6. **`--hours-back` flag** â€“ fetcher defaults to 6 h, overridable.
7. **Rule threshold raised** â€“ already set to 14 (done).
8. **Daily cap settable** â€“ currently 20 for smoke-test; environment variable override planned.

These items ensure no data loss, immediate key validation, clear terminal feedback, and flexible local serving for v3 tests. 